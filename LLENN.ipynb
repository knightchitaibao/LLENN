{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightning import LightningModule, Trainer\n",
    "import lightning as L\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([50000, 256]), torch.float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = r'.\\Dataset\\datav1.pkl'\n",
    "with open(file , 'rb') as f:\n",
    "    rdata = pkl.load(f)\n",
    "\n",
    "type(rdata), rdata.shape, rdata.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 128, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdata = torch.concat([rdata[::1, :128].unsqueeze(dim=2), rdata[::1, 128:].unsqueeze(dim=2)], dim=2)\n",
    "rdata.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 32\n",
    "if precision == 32:\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "elif precision == 64:\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    data = rdata.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdatamin = rdata.min()\n",
    "data = torch.log10(rdata - rdatamin + 1)[::200]\n",
    "mindata = data.min()\n",
    "data = (data - data.mean())/data.std()\n",
    "#data = (data - data.min())/(data.max() - data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lle_ds(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.nst, self.np, self.n = data.size()\n",
    "        self.np = self.np\n",
    "        self.slowtimes = torch.linspace(0, 1, self.nst)\n",
    "        self.phases = torch.linspace(0, 1, self.np)\n",
    "        self.Xs = torch.stack([self.slowtimes.unsqueeze(1).repeat(1, self.np), \n",
    "                         self.phases.unsqueeze(0).repeat(self.nst, 1)], dim=-1)\n",
    "        self.Xs = self.Xs.reshape(-1, 2)\n",
    "        self.lle_Ams = data.reshape(-1, 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nst*self.np\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.Xs[index]\n",
    "        Am = self.lle_Ams[index]\n",
    "        return X, Am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(LightningModule):\n",
    "    def __init__(self, lr=None, bs=None, \n",
    "                 nMLP=None, lMLP=None,\n",
    "                 loss_fn=None,\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.learning_rate = lr\n",
    "        self.batch_size = bs\n",
    "        self.save_hyperparameters()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(2, nMLP),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(nMLP, nMLP),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(nMLP, int(nMLP/2)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(int(nMLP/2), 2),\n",
    "        )\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        MLPout = self.MLP(x)\n",
    "        #MLPout (bs, sl, freqs)\n",
    "        return MLPout\n",
    "    \n",
    "    def assemble_ds(self, ds):\n",
    "        trnlen = int(0.9*len(ds))\n",
    "        print(trnlen)\n",
    "        self.trnset = ds\n",
    "        self.valset = ds[trnlen:]\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, Y = batch\n",
    "        op = self.forward(X)\n",
    "        loss = self.loss_fn(op, Y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, Y = batch\n",
    "        op = self.forward(X)\n",
    "        loss = self.loss_fn(op, Y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataloader = DataLoader(self.trnset, batch_size=self.batch_size)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = DataLoader(self.valset, batch_size=self.batch_size)\n",
    "        return dataloader\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        from torch.optim import AdamW, Adam, SGD\n",
    "        optim = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "bs = 1028\n",
    "nMLP = 32\n",
    "lMLP = 1\n",
    "loss_fn = F.mse_loss\n",
    "epo = 100\n",
    "\n",
    "ds = lle_ds(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'Simple'\n",
    "name = f'nMLP_{nMLP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28800\n"
     ]
    }
   ],
   "source": [
    "version = f'v1'\n",
    "logger = TensorBoardLogger(save_dir, name=name, version=version)\n",
    "ckpt = ModelCheckpoint(\n",
    "            os.path.join(save_dir, name, version), monitor='val_loss',\n",
    "            filename='MLP_epoch_{epoch:03d}_valloss{val_loss:.5f}'\n",
    "        )\n",
    "model = PINN(lr, bs, nMLP, lMLP, loss_fn)\n",
    "model.assemble_ds(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "d:\\anaconda\\envs\\dl\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory D:\\Githome\\LLENN\\Simple\\nMLP_32\\v1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | MLP  | Sequential | 1.7 K  | train\n",
      "--------------------------------------------\n",
      "1.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\dl\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\dl\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "d:\\anaconda\\envs\\dl\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 32/32 [00:00<00:00, 113.27it/s, v_num=v1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 32/32 [00:00<00:00, 112.88it/s, v_num=v1]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=epo, check_val_every_n_epoch=2, callbacks=[ckpt], logger=logger)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
